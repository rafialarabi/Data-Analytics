# -*- coding: utf-8 -*-
"""Assignment_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15z6XtcQvz5oNDrVbbZxK5yiXIEiGpbOz

Part 1
"""

from google.colab import files
my_file = files.upload()

import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

# Load the Titanic dataset
titanic_df = pd.read_csv('titanic.csv')

# Display the first few rows of the dataset
print(titanic_df.head())

# Calculate basic statistics for numeric columns
numeric_stats = titanic_df.describe()
print(numeric_stats)

# Visualization of the statistics can be done using a bar chart or box plot.

columns_for_boxplot = ["Age", "Parch", "Fare"]

# Creating box plots using Seaborn
plt.figure(figsize=(12, 8))
sns.set(style="whitegrid")  # Set the style of the grid

# Creating box plots for each numerical column
sns.boxplot(data=titanic_df[columns_for_boxplot], orient="h", palette="Set2")
plt.title("Box Plots for Numeric Statistics")
plt.xlabel("Value")
plt.show()

"""Part 2"""

import matplotlib.pyplot as plt

# Count passengers in each class
class_counts = titanic_df['Pclass'].value_counts()

# Create a bar chart
plt.bar(class_counts.index, class_counts.values)
plt.xlabel('Passenger Class')
plt.ylabel('Count')
plt.title('Distribution of Passengers by Class')
plt.xticks(class_counts.index, ['1st', '2nd', '3rd'])
plt.show()

"""Observation: The majority of passengers were in the 3rd class, followed by the 1st class, and the 2nd class had the fewest passengers."""



"""Part 3"""

# Create a histogram
plt.hist(titanic_df['Age'], bins=20, edgecolor='k')
plt.xlabel('Age')
plt.ylabel('Count')
plt.title('Age Distribution of Passengers')
plt.show()

"""Observation: The age distribution appears to be slightly right-skewed, with a concentration of passengers in the 20-30 age range.

Part 4
"""

# Calculate survival rate
survival_counts = titanic_df['Survived'].value_counts()
survival_rate = survival_counts[1] / len(titanic_df) * 100

# Create a pie chart
labels = ['Not Survived', 'Survived']
sizes = [survival_counts[0], survival_counts[1]]
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
plt.title('Survival Rate of Passengers')
plt.show()

"""Observation: Approximately 38% of passengers survived.

Part 5
"""

# Count passengers by gender
gender_counts = titanic_df['Sex'].value_counts()

# Create a bar chart
plt.bar(gender_counts.index, gender_counts.values)
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Count of Passengers by Gender')
plt.show()

"""Observation: There are more male passengers than female passengers.

Part 6
"""

# Calculate survival counts by gender
survival_by_gender = titanic_df.groupby(['Sex', 'Survived']).size().unstack()

# Create a stacked bar chart
survival_by_gender.plot(kind='bar', stacked=True)
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Survival by Gender')
plt.legend(['Not Survived', 'Survived'])
plt.show()

"""Observation: More females survived compared to males.

Part 7
"""

# Create a count plot
sns.countplot(data=titanic_df, x='Embarked')
plt.xlabel('Embarkation Point')
plt.ylabel('Count')
plt.title('Number of Passengers by Embarkation Point')
plt.show()

"""Observation: Most passengers embarked from Southampton (S), followed by Cherbourg (C), and Queenstown (Q).

Part 8
"""

titanic_df['Parch']

"""Observation: There are some outliers in family size, with most passengers having smaller families."""

## WRONG - GIVING MARKS TO EVERYONE
sns.boxplot(data = titanic_df, x = "Parch", y = "PassengerId")
plt.title("Box Plot family size")
plt.xlabel("Number of Children")
plt.ylabel("Number of Passenger")
plt.show()

"""Part 9"""

# Create a scatter plot
plt.scatter(titanic_df['Age'], titanic_df['Fare'], alpha=0.5)
plt.xlabel('Age')
plt.ylabel('Fare')
plt.title('Scatter Plot of Fare vs. Age')
plt.show()

"""Observation: There doesn't appear to be a strong correlation between fare and age.

Part 10

To create a multi-bar chart (grouped bar chart) showing the survival rate by passenger class and age group, you can use similar code as in Part 8, but categorize passengers into age groups and compare survival rates across classes.
"""



"""# Question 2"""

from google.colab import files
uploaded = files.upload()

import numpy as np
import pandas as pd

import re
import string

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer


nltk.download('stopwords')

def cleaning_with_re(tweet):
    try:
      # remove hyperlinks
      tweet = re.sub(r'https?://(?:www\.[^\s\n\r]+|[^\s\n\r]+)', '', tweet)

      # remove hashtag (#)
      tweet = re.sub(r'#', '', tweet)

      #replaces newline (\n) and carriage return (\r) characters in a tweet with an empty string
      tweet = re.sub(r'[\n\r]', '', tweet)
    except:
      print(tweet)

    return tweet

def tokenizing(tweet):

    stopwords_english = stopwords.words('english')
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)
    new_tweet = tokenizer.tokenize(tweet)

    clean_tweet=[]

    for word in new_tweet: # Going through every word in tokens list
        if (word not in stopwords_english and word not in string.punctuation):  # remove punctuation and stopwords
            clean_tweet.append(word)

    return clean_tweet

def stemming(tweet):
    # Instantiate stemming class
    stemmer = PorterStemmer()

    # Create an empty list to store the stems
    stemmed_tweets = []

    for word in tweet:
        stem_word = stemmer.stem(word)  # stemming word
        stemmed_tweets.append(stem_word)  # append to the list

    return stemmed_tweets

def make_str(tweet):
    new = ""
    for item in tweet:
        new += (item + " ")
    return new

def preprocessing(tweet):
    tweet=cleaning_with_re(tweet)
    tweet=tokenizing(tweet)
    tweet=stemming(tweet)
    tweet = make_str(tweet)

    return tweet

df = pd.read_csv("/content/Corona_NLP.csv")

df.head()

# 2.1.2
df.tail(10)

# 2.2.1
df.duplicated().sum()

print(df[df.duplicated()])

# 2.2.1
df = df.drop_duplicates()

#2.2.2a
# Get unique values and their frequencies from the 'Location' column
unique_values = df['Location'].value_counts()

# Print unique values and their frequencies
print("Unique values and their frequencies in the 'Location' column:")
print(unique_values)

# 2.2.2b
unique_values.sort_values(ascending=False)

# 2.2.2c
top_locations = unique_values.head(20)
plt.figure(figsize=(12, 8))
top_locations.plot(kind='bar', color='skyblue')
plt.xlabel('Location')
plt.ylabel('Frequency')
plt.title('Top 20 Locations and Their Frequencies')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# 2.2.3
# Define an appropriate color palette for the sentiment categories
colors = {"Negative": "#d73027", "Extremely Negative": "#a50026",
          "Positive": "#4575b4", "Extremely Positive": "#91bfdb", "Neutral": "#999999"}


# Visualize the distribution of Sentiment using a countplot with the specified color palette
plt.figure(figsize=(8, 6))
sns.countplot(x='Sentiment', data=df, palette=colors, order=["Extremely Negative", "Negative", "Neutral", "Positive", "Extremely Positive"])
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.title('Sentiment Distribution')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 2.2.4
df['Sentiment'] = df['Sentiment'].replace({'Extremely Negative': 'Negative', 'Extremely Positive': 'Positive'})

df.head()

df = df.dropna(subset=['OriginalTweet', 'Sentiment'])

count_nan = df.isna().sum()

count_nan

df["cleaned_tweet"] = df['OriginalTweet'].apply(preprocessing)

df.head()

df = df.dropna(subset=["cleaned_tweet"])

X = df['cleaned_tweet']
y = df['Sentiment']

type(df["cleaned_tweet"][0])

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

df.shape

tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # You can adjust the number of features as needed
X_tfidf = tfidf_vectorizer.fit_transform(X)

# Step 3: Splitting Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Step 4: Model Building (Logistic Regression)
classifier = LogisticRegression(max_iter=1000)

# Step 5: Training the Model
classifier.fit(X_train, y_train)

# Step 6: Model Evaluation
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('q3_data.csv')
df.head()

df.describe()

x1 = df['x1']
x2 = df['x2']
# Plot the data
plt.scatter(x1, x2, alpha=0.5)
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Scatter Plot of x1 and x2 with Varying Correlation')
plt.show()

from scipy.stats import linregress

# Fit a linear regression line
slope, intercept, r_value, p_value, std_err = linregress(x1, x2)

# Calculate the predicted values using the regression line
predicted_x2 = slope * x1 + intercept

# Plot the data points and the regression line
plt.scatter(x1, x2, alpha=0.5, label='Data')
plt.plot(x1, predicted_x2, color='red', label='Regression Line')
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Regression Line and Scatter Plot of x1 and x2')
plt.legend()

# Calculate and print the correlation coefficient (r-value)
print(f'Correlation Coefficient (r-value): {r_value:.4f}')

# Show the plot
plt.show()

"""Correlation coefficient suggests a strong negative correlation (1)

Looking at the scatter plot and regression line, we see that there is in fact two clusters, one with positive correlation and one with negative correlation. Looking at only the correlation coefficient is therefore subject to misinterpretation of the distribution (2)
"""

